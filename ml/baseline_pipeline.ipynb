{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the trace data into a string to be used in the CountVectorizer\n",
    "class ExecTraceTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "        return X.apply(lambda traces: ' '.join(traces) if isinstance(traces, list) else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier': [LogisticRegression(max_iter=5000)],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def get_logs(logs_dir, mutation_index):\n",
    "    files = os.listdir(logs_dir)\n",
    "    logs = []\n",
    "    for file_name in files:\n",
    "        if file_name.startswith(f\"mutation{mutation_index}_\"):\n",
    "            with open(os.path.join(logs_dir, file_name), \"r\") as f:\n",
    "                logs.append(json.load(f))\n",
    "    # Count the number of logs\n",
    "    print(f\"Number of logs: {len(logs)}\")\n",
    "    return logs\n",
    "\n",
    "# Combine logs into DataFrame\n",
    "def combine_logs(logs):\n",
    "    combined_logs = [log for log in logs if isinstance(log, dict)]\n",
    "    df = pd.DataFrame(combined_logs)\n",
    "    return df\n",
    "\n",
    "# Build pipeline for model\n",
    "def build_pipeline(classifier, feature_selector=SelectKBest(score_func=chi2, k=5)):\n",
    "    # Pipeline for preprocessing text in 'exec_trace' column\n",
    "    exec_trace_pipeline = Pipeline([\n",
    "        ('exec_transform', ExecTraceTransformer()),\n",
    "        ('vectorizer', CountVectorizer())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('exec_trace', exec_trace_pipeline, 'exec_trace'),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    # Pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Train model, hyperparameter tuning with GridSearchCV\n",
    "def train_model(X_train, y_train, pipeline, param_grid):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "def save_model(model, filename):\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"Model saved as '{filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutationModelTrainer:\n",
    "    def __init__(self, base_dir, logs_subdirs_to_mutations, param_grid, model_save_dir=\"models\"):\n",
    "        self.base_dir = base_dir\n",
    "        self.logs_subdirs_to_mutations = logs_subdirs_to_mutations\n",
    "        self.param_grid = param_grid\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.results = {}\n",
    "        \n",
    "        os.makedirs(self.model_save_dir, exist_ok=True)\n",
    "    \n",
    "    def process_mutation(self, logs_subdir, mutation_index):\n",
    "        logs_dir = os.path.join(self.base_dir, logs_subdir)\n",
    "        print(f\"\\nProcessing Logs Subdir: '{logs_subdir}', Mutation Index: {mutation_index}\")\n",
    "        \n",
    "        project_name = logs_subdir.split('/')[-4]\n",
    "        print(f\"Project Name: {project_name}\")\n",
    "        \n",
    "        logs = get_logs(logs_dir, mutation_index)\n",
    "\n",
    "        if not logs:\n",
    "            print(f\"No logs found for mutation index {mutation_index} in '{logs_subdir}'. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        df = combine_logs(logs)\n",
    "        \n",
    "        df = df.dropna(subset=['exec_trace', 'verdict'])\n",
    "        \n",
    "        y = df['verdict'].apply(lambda x: 1 if x.lower() == 'pass' else 0)\n",
    "        \n",
    "        X = df[['exec_trace']]\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Build the pipeline\n",
    "        placeholder_classifier = LogisticRegression()\n",
    "        pipeline = build_pipeline(classifier=placeholder_classifier)\n",
    "        \n",
    "        # Train the model\n",
    "        grid_search = train_model(X_train, y_train, pipeline, self.param_grid)\n",
    "        \n",
    "        # Evaluate the best model\n",
    "        evaluation = evaluate_model(grid_search.best_estimator_, X_test, y_test)\n",
    "        \n",
    "        # Save the best model     \n",
    "        model_filename = os.path.join(self.model_save_dir, f\"{project_name}_best_pipeline_mutation_{mutation_index}.pkl\")\n",
    "        save_model(grid_search.best_estimator_, model_filename)\n",
    "        \n",
    "        self.results[mutation_index] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'test_accuracy': evaluation['accuracy'],\n",
    "            'classification_report': evaluation['classification_report']\n",
    "        }\n",
    "        \n",
    "        print(\"Best Parameters:\")\n",
    "        print(grid_search.best_params_)\n",
    "        print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Test Set Accuracy: {evaluation['accuracy']:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(evaluation['classification_report'])\n",
    "    \n",
    "    def train_all(self):\n",
    "        for logs_subdir, mutation_indices in self.logs_subdirs_to_mutations.items():\n",
    "            for mutation_index in mutation_indices:\n",
    "                self.process_mutation(logs_subdir, mutation_index)\n",
    "    \n",
    "    def get_results(self):\n",
    "        return self.results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Logs Subdir: 'textdistance/test_DamerauLevenshtein/logs/logs', Mutation Index: 2\n",
      "Project Name: textdistance\n",
      "Number of logs: 10000\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Model saved as 'models\\textdistance_best_pipeline_mutation_2.pkl'\n",
      "Best Parameters:\n",
      "{'classifier': LogisticRegression(max_iter=5000)}\n",
      "Best Cross-Validation Score: 0.7614\n",
      "Test Set Accuracy: 0.7630\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.79      1052\n",
      "           1       0.81      0.65      0.72       948\n",
      "\n",
      "    accuracy                           0.76      2000\n",
      "   macro avg       0.77      0.76      0.76      2000\n",
      "weighted avg       0.77      0.76      0.76      2000\n",
      "\n",
      "\n",
      "Processing Logs Subdir: 'dateutil/test_date_parse/logs/logs', Mutation Index: 3\n",
      "Project Name: dateutil\n",
      "Number of logs: 10000\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Model saved as 'models\\dateutil_best_pipeline_mutation_3.pkl'\n",
      "Best Parameters:\n",
      "{'classifier': LogisticRegression(max_iter=5000)}\n",
      "Best Cross-Validation Score: 0.9605\n",
      "Test Set Accuracy: 0.9575\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.80       251\n",
      "           1       0.95      1.00      0.98      1749\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.98      0.83      0.89      2000\n",
      "weighted avg       0.96      0.96      0.95      2000\n",
      "\n",
      "\n",
      "Mutation 2:\n",
      "Best Parameters: {'classifier': LogisticRegression(max_iter=5000)}\n",
      "Best CV Score: 0.7614\n",
      "Test Accuracy: 0.7630\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.79      1052\n",
      "           1       0.81      0.65      0.72       948\n",
      "\n",
      "    accuracy                           0.76      2000\n",
      "   macro avg       0.77      0.76      0.76      2000\n",
      "weighted avg       0.77      0.76      0.76      2000\n",
      "\n",
      "\n",
      "Mutation 3:\n",
      "Best Parameters: {'classifier': LogisticRegression(max_iter=5000)}\n",
      "Best CV Score: 0.9605\n",
      "Test Accuracy: 0.9575\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.80       251\n",
      "           1       0.95      1.00      0.98      1749\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.98      0.83      0.89      2000\n",
      "weighted avg       0.96      0.96      0.95      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../fuzz_test\"\n",
    "\n",
    "# Subdirectory path: [mutation indices]\n",
    "logs_subdirs_to_train = {\n",
    "    \"textdistance/test_DamerauLevenshtein/logs/logs\": [2],\n",
    "    \"dateutil/test_date_parse/logs/logs\": [3],\n",
    "}\n",
    "\n",
    "model_save_dir = \"models\"\n",
    "\n",
    "trainer = MutationModelTrainer(\n",
    "    base_dir=base_dir,\n",
    "    logs_subdirs_to_mutations=logs_subdirs_to_train,\n",
    "    param_grid=param_grid,\n",
    "    model_save_dir=model_save_dir\n",
    ")\n",
    "\n",
    "trainer.train_all()\n",
    "\n",
    "results = trainer.get_results()\n",
    "\n",
    "for mutation, res in results.items():\n",
    "    print(f\"\\nMutation {mutation}:\")\n",
    "    print(f\"Best Parameters: {res['best_params']}\")\n",
    "    print(f\"Best CV Score: {res['best_score']:.4f}\")\n",
    "    print(f\"Test Accuracy: {res['test_accuracy']:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(res['classification_report'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
